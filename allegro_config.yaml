# -----------------------------------------------------------
#  Allegro / NequIP training 
# -----------------------------------------------------------

run: [train, test]

# ----------------------- global hyper-parameters ---------------------------
cutoff_radius: 4.10                 # Å – neighbour-list cutoff
chemical_symbols: [C, O]            # dataset contains carbon and oxygen only
model_type_names: ${chemical_symbols}
monitored_metric: val0_epoch/weighted_sum

num_scalar_features: 64             # "width" of all scalar MLPs

# ---------------------------- data module ---------------------------------
data:
  _target_: nequip.data.datamodule.ASEDataModule
  seed: 696969

  split_dataset:
    file_path: raw_data/vasp_frames.extxyz   # path is relative to run_allegro.sh CWD
    train: 0.90
    val:   0.10
    test:  0.00

  transforms:
    - _target_: nequip.data.transforms.NeighborListTransform
      r_max: ${cutoff_radius}
    - _target_: nequip.data.transforms.ChemicalSpeciesToAtomTypeMapper
      chemical_symbols: ${chemical_symbols}

  train_dataloader:
    _target_: torch.utils.data.DataLoader
    batch_size: 4                       
    num_workers: 10                     # matches OMP_NUM_THREADS in run_allegro.sh

  val_dataloader:
    _target_: torch.utils.data.DataLoader
    batch_size: 4
    num_workers: 10 

  test_dataloader: ${data.val_dataloader}

  stats_manager:
    _target_: nequip.data.CommonDataStatisticsManager
    type_names: ${model_type_names}

# ----------------------- PyTorch-Lightning trainer ------------------------
trainer:
  _target_: lightning.Trainer
  max_epochs: 125 
  check_val_every_n_epoch: 1
  log_every_n_steps: 125

  callbacks:
    - _target_: lightning.pytorch.callbacks.EarlyStopping
      monitor: ${monitored_metric}
      min_delta: 1.0e-4
      patience: 30

    - _target_: lightning.pytorch.callbacks.ModelCheckpoint
      monitor: ${monitored_metric}
      dirpath: ${hydra:runtime.output_dir}
      filename: best
      save_last: true

    - _target_: lightning.pytorch.callbacks.LearningRateMonitor
      logging_interval: epoch

  logger:
    _target_: lightning.pytorch.loggers.CSVLogger
    name: allegro_logs
    save_dir: ${hydra:runtime.output_dir}

# ---------------------------- model & loss --------------------------------
training_module:
  _target_: nequip.train.EMALightningModule

  loss:
    _target_: nequip.train.EnergyForceLoss
    per_atom_energy: true
    coeffs:
      total_energy: 1.0
      forces: 10.0

  val_metrics:
    _target_: nequip.train.EnergyForceMetrics
    coeffs:
      per_atom_energy_mae: 1.0
      forces_mae: 10.0

  test_metrics: ${training_module.val_metrics}

  optimizer:
    _target_: torch.optim.Adam
    lr: 0.001

  lr_scheduler:
    scheduler:
      _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
      factor: 0.5
      patience: 5
      threshold: 0.02
      min_lr: 1.0e-7
    monitor: ${monitored_metric}
    interval: epoch
    frequency: 1

  # ------------------------ Allegro network block -------------------------
  model:
    _target_: allegro.model.AllegroModel

    # ---- basics ----
    seed: 456
    model_dtype: float32
    type_names: ${model_type_names}
    r_max: ${cutoff_radius}

    # ---- two-body radial-chemical embedding ----
    radial_chemical_embed:
      _target_: allegro.nn.TwoBodyBesselScalarEmbed
      num_bessels: 8
      bessel_trainable: true
      polynomial_cutoff_p: 6
    radial_chemical_embed_dim: ${num_scalar_features}

    # ---- scalar embedding MLP ----
    scalar_embed_mlp_hidden_layers_depth: 2
    scalar_embed_mlp_hidden_layers_width: ${num_scalar_features}
    scalar_embed_mlp_nonlinearity: silu

    # ---- core hyper-parameters ----
    l_max: 2
    num_layers: 2
    num_scalar_features: ${num_scalar_features}
    num_tensor_features: 32

    # ---- Allegro MLPs ----
    allegro_mlp_hidden_layers_depth: 2
    allegro_mlp_hidden_layers_width: ${num_scalar_features}
    allegro_mlp_nonlinearity: silu

    # ---- advanced flags ----
    parity: false
    tp_path_channel_coupling: true

    # ---- readout MLP ----
    readout_mlp_hidden_layers_depth: 2
    readout_mlp_hidden_layers_width: ${num_scalar_features}
    readout_mlp_nonlinearity: silu

    # ---- per-type scales & misc ----
    avg_num_neighbors: ${training_data_stats:num_neighbors_mean}
    per_type_energy_shifts: [0.00943113, -0.02716954]
    per_type_energy_scales: ${training_data_stats:forces_rms}
    per_type_energy_scales_trainable: true
    per_type_energy_shifts_trainable: false

# ----------------- global backend / precision switches --------------------
global_options:
  allow_tf32: true
